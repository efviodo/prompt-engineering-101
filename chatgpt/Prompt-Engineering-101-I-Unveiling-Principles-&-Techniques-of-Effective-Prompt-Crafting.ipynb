{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f5976c5",
   "metadata": {},
   "source": [
    "<img src=\"images/1.jpeg\" alt=\"Header\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0fe7b8",
   "metadata": {},
   "source": [
    "# Prompt Engineering 101 - I: Unveiling Principles & Techniques of Effective Prompt Crafting\n",
    "\n",
    "The cover image features the [Commodore PET (Personal Electronic Transactor)](https://es.wikipedia.org/wiki/Commodore_PET?ref=hackernoon.com), a revolutionary machine launched in 1977 and presage of a new era of personal computing. Despite its modest 1MHz microprocessor, 8KB of RAM, and built-in monochrome monitor, marked a pivotal shift, taking computers from a do-it-yourself computer assembly hobbyist culture, to any middle class family (considering that it costed the equivalent of USD 3500 now).\n",
    "\n",
    "GPT-3 and later models like **ChatGPT** or **StableDifussion**, had a similar transformative effect on AI. Now you don't need to be an engineer well-versed in linear algebra to have access to powerful Machine Learning models, just how to write **clear instructions**.\n",
    "\n",
    "Even so, from the eyes of many people, ChatGPT and Generative AI is still a magic box that achieves incredible things in ways that no one understands, and this leads many people to stay away from this technology. Surprisingly, the same thing happened with early personal computers like the Commodore. Nevertheless, there is nothing magical and nothing to fear about Language Models, there are just statistical models that predict the next most probable token based on a context input. Statistical models and very powerful as a tool for many common tasks.\n",
    "\n",
    "\n",
    "**Prompt Engineering 101** is a post series designed and written to unveiling the principles and techniques of prompt engineering, the art of crafting clear and effective texts, to prompt language models and get exactly what you are looking for. Furthermore, this series is not restricted to autoregressive language models like ChatGPT, rather aims to address Prompt Engineering as a general technique to any Generative models, including text-to-image models such as Stable Diffusion or Midjourney. In addition, address limitations of Language Models like hallucinations, techniques to prevent them, privacy and security concerns, and more…\n",
    "\n",
    "On the other hand, there are a variety of tutorials on Prompt Engineering: blogs, boot camps in LLMs, reading lists, etc. This series does not pretend to be the best source or an ultimate reference. Far from it, it merely intends to offer a complementary, practical, and easy-to-read look to this subject.\n",
    "\n",
    "\n",
    "In post #1 of this series, we covered basic principles and tactics for writing clear and specific prompts for text-to-text models (like ChatGPT). I hope you enjoy it!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f9cc9f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Note:</b> When I wrote this notebook, the most recent version of GPT available through the OpenAI API was the <b>gpt-3.5-turbo</b> model (as you can notice in the code). However, <b>GPT-4</b> was already available through the Plus version of the <b>ChatGPT</b> application and since I was interested in testing Prompt Engineering principles with the most powerful model available, I use ChatGPT Plus interface in most of the experiments I ran. \n",
    "In conclusion, you may notice some major differences between the displayed output and the one you will get if you use the <b>gpt-3.5-turbo</b> model. Also, I had to use <b>gpt-3.5-turbo</b> via API to play with the <i>Top P</i> and <i>Temperature</i> parameters, so you may notice a decrease in the language creativity, that the model expresses in these specific prompts.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1103cee2",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [What is Prompt Engineering](#what-is-prompt-engineering)\n",
    "2. [Principles of Prompt Engineering](#principles-of-prompt)\n",
    "3. [Five Tips & Tricks to Make your Prompt Effective](#five-tips-and-tricks)\n",
    "4. [Wrapping Up](#wrapping-up)\n",
    "5. [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a262a39",
   "metadata": {},
   "source": [
    "## Imports & Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f50d22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')  # Be sure you have a .env file defined with your OPENAI_API_KEY \n",
    "\n",
    "PROMPT_FOLDER = \"prompts\"\n",
    "OUTPUT_FOLDER = \"output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eabdf860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=\"gpt-3.5-turbo\", temperature=1.0, top_p=1.0):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        top_p=top_p\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def save(path: str, text: str):\n",
    "    \"\"\"\n",
    "    Helper function to save prompts and model responses as TXT files\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(f\"{path}.txt\", \"w\") as text_file:\n",
    "        text_file.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f5cc4f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "\n",
       ".label {\n",
       "  color: white;\n",
       "  padding: 5px;\n",
       "}\n",
       "\n",
       ".success {background-color: #04AA6D;} /* Green */\n",
       ".info {background-color: #2196F3;} /* Blue */\n",
       "\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Styles \n",
    "\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"\n",
    "<style>\n",
    "\n",
    ".label {\n",
    "  color: white;\n",
    "  padding: 5px;\n",
    "}\n",
    "\n",
    ".success {background-color: #04AA6D;} /* Green */\n",
    ".info {background-color: #2196F3;} /* Blue */\n",
    "\n",
    "</style>\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1c466d",
   "metadata": {},
   "source": [
    "## What is Prompt Engineering? \n",
    "<a class=\"anchor\" id=\"what-is-prompt-engineering\"></a>\n",
    "\n",
    "**From intuition**, a prompt is nothing more than a text that goes inside a Language Model (LM), and “Prompt Engineering” is the art of designing that text to get the output we want. Some people joke that Prompts are like magic spells: a collection of words, that follows bizarre and complex rules, that achieves the impossible.\n",
    "\n",
    "Despite the lovely metaphor, there is nothing magical about Prompt Engineering. Language Models are just statistical models for text, and prompt engineering is just a technique to design the input text that contains the precise tokens (for example instructions and context) that orient the model in the right direction to produce a good result (for example more tokens or an image). In addition, this technique can be defined through simple rules, learned, and ultimately mastered.\n",
    "\n",
    "\n",
    "**Formally**, Prompt Engineering is a comprehensive process of designing and optimizing specific and unambiguous prompts for a Large Language Model to ensure that it generates relevant, accurate, and coherent responses. It involves tailoring input prompts that are clear, concise, and also includes enough context in order to the model to understand the user's input and generate an appropriat?ref=hackernoon.come response. It also involves a deep understanding of the capabilities and limitations of the model, and requires iterations and testing to reach the desired outcome.\n",
    "\n",
    "\n",
    "In essence, the iterative prompt development process is very similar to writing code. If it doesn't work, we refine and retry until we get the desired result.\n",
    "\n",
    "\n",
    "Effective prompt engineering is essential for building conversational agents that can understand and respond to natural language inputs, in a way that is useful and engaging for human users. Furthermore is of vital importance to prevent situations where the model may generate nonsensical, inappropriate or harmful responses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3870acb7",
   "metadata": {},
   "source": [
    "## Principles of Prompt Engineering\n",
    "<a class=\"anchor\" id=\"principles-of-prompt\"></a>\n",
    "\n",
    "There are three principles behind writing effective prompts that will lead you to get the output you expect quickly and with no pain.\n",
    "\n",
    "<img src=\"images/2.png\" alt=\"Principles of Prompt Engineering\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d42f91",
   "metadata": {},
   "source": [
    "## Principle 1: Write clear and specific instructions\n",
    "\n",
    "The best way to obtain the desired model output is to provide instructions that are as clear and specific as you can possibly make. This reduces the chances to get irrelevant or incorrect responses. However, don't confuse writing clear instructions with writing short instructions. In some cases, longer prompts actually provide more clarity and context information that will lead the model to generate more detailed and accurate responses.\n",
    "\n",
    "Here are some tactics you can follow to get clear and specific prompts.\n",
    "\n",
    "### 1. Use delimiters\n",
    "Use delimiters in your prompt to separate the prompt instructions (system instructions) from the user input. You can use any punctuation symbol that clearly tell the model the separation between the instructions and input.\n",
    "\n",
    "* Triple quotes `“““`\n",
    "* Triple back-ticks \\`\\`\\`\n",
    "* Triple dashes `---`\n",
    "* Angle brackets `< >`\n",
    "* XML tags `<tag> </tag>`\n",
    "\n",
    "Remember that long or verbose delimiters generates longer prompts (in tokens number), which can be a problem on some models with a short value of maximum prompt length.\n",
    "\n",
    "Using delimiters is also a helpful technique to avoid prompting injections by explicitly separating the model instructions from the rest of the prompt (I will cover this in more detail later in this post series)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd4b571",
   "metadata": {},
   "source": [
    "Let's see this tactic in practice with a real example. Imagine we are working to improve an e-commerce website providing better insights to the sales team based on customer reviews. For instance, if shoppers frequently mention that a product's sizing is either too large or too small, this could indicate that the actual size chart is inaccurate.\n",
    "\n",
    "However, manually analyzing customer reviews is a tedious and time-consuming task. For this reason, we will use GPT-4 to examine these reviews and provide a comprehensive summary.\n",
    "\n",
    "The following examples are actual reviews that online shoppers have given of a [Columbia Men's Republic Parka (Obi-Wan Kenobi™)](https://www.columbia.com/p/mens-republic-parka-obi-wan-kenobi-2049261_OWK.html). Which BTW looks soo cool!! If it didn't cost 500 bucks I would buy it too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc94e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = \"\"\"\n",
    "1 Star\n",
    "This is an over engineered waste of money\n",
    "\n",
    "Nobody is truly going to be wearing this unless your in the frozen tundra…. Zero mobility way to large in length… Absolute waste…\n",
    "\n",
    "---\n",
    "5 Stars\n",
    "Incredibly warm!\n",
    "\n",
    "Incredibly warm! You won’t go wrong with this jacket!\n",
    "\n",
    "---\n",
    "5 Stars\n",
    "So warm!\n",
    "\n",
    "I am a 5”4 woman and got this men’s jacket as a gift. I moved from sunny Los Ángeles to Detroit Michigan during the storm and this jacket saved me. I get cold easily and I was so warm while walking in the snow that I had to unzip a bit because I was getting too warm. Love that this jacket has so many pockets, I don’t even need to bring a purse cuz this jacket holds it all. The details they put into this jacket is amazing.\n",
    "\n",
    "---\n",
    "1 Star\n",
    "Waste of money great look but just not designed to be used and it’s shown by the resale value\n",
    "\n",
    "Absolutely way too bulky and does not fit as described… This jacket is over engineered meaning unless your going to the polar ice caps you won’t need it. Was hoping for a solid winter jacket and this is definitely not it. Zero mobility in this with how bulky it is… And I have other jackets by Columbia… Also Ordered an XL just as my other jackets but this one is basically a XXL the jacket is also extremely long for no reason. Definitely will be returning this as the package also came opened and the jacket was dirty.\n",
    "\n",
    "---\n",
    "2 Stars\n",
    "Overpriced, and runs big\n",
    "\n",
    "I love Star Wars and really loved this jacket in concept. It took forever to ship and when it did finally show up the package was ripped open and the jacket was dirty and damaged. Customer service was not helpful and I kept getting kicked from chat. They wanted me to return it and rebuy while doing so meanwhile I have spent $1000 and have no jacket for most of winter. Not worth the money or the hassle. Guess I am still a north face guy.\n",
    "\n",
    "---\n",
    "5 Stars\n",
    "I love it!!\n",
    "\n",
    "A very nice parka!! Comfy and warm and it looks great! Very glad I purchased it.\n",
    "\n",
    "---\n",
    "5 Stars\n",
    "Awesome\n",
    "\n",
    "Warmest jacket I've ever had. Lots of pockets, looks excellent\n",
    "\n",
    "---\n",
    "2 Stars\n",
    "Very nice looking jacket\n",
    "\n",
    "The jacket looks really nice and is warm . The sizing chart is completely off. I would normally be a large to extra large because of wide shoulders and back (I matched the large I'm the size chart). I can almost fit 2 of me into this jacket . Definitely go a size down , maybe even 2 . If you an athletic build .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785d0b79",
   "metadata": {},
   "source": [
    "Let's start with a simple and specific prompt, indicating the model to analyze the customer's review and generate an overview of them. In the spirit of making more clean and easy-to-read examples, we will separate the system instructions from the rest of the input. To do this, we will use Python [string literals](https://docs.python.org/3/reference/lexical_analysis.html?ref=hackernoon.com#formatted-string-literals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5776af1",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fb13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a list of product reviews delimited by <>.\n",
    "Analyze each reviews and provide an overview of customers satisfaction, listing \n",
    "positive and negative aspects reported by customers. \n",
    "\n",
    "<{reviews}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35062d4",
   "metadata": {},
   "source": [
    "Our initial prompt is extremely concise and specific, with <> to separate the instructions from the rest of the input (you can use back-ticks, triple quotes or any other character) . Despite its simplicity, the model gets the idea and generates a correct response, identifying and enumerating positives and negatives aspects mentioned by shoppers in their reviews. In particular, the model properly identifies highlighted aspects by shoppers like ample storage space, comfortable and warm feeling, among others. Analogous to positive aspects, recognize a few customers complains such as sizing issues and shipping issues.\n",
    "\n",
    "Notice that the model cites some aspects that are not strictly included in the customer reviews. For instance: ample storage space. Customers mentioned multiple pockets but never said plainly \"ample\" (this is an implicit assumption). Also, none complain specifically about the jacket price. This discrepancy and inconsistencies are called [Hallucinations](https://en.wikipedia.org/wiki/Hallucination_(artificial_intelligence)?ref=hackernoon.com), is a [common issue](https://spectrum.ieee.org/ai-hallucination?ref=hackernoon.com) in LLM. We will see later a tactic that helps to reduce it. Feel free to read through the customer's review and take your own insights, then compare them with the generated by the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce63bf3e",
   "metadata": {},
   "source": [
    "**Response 🤖** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4583dad",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Overview of customer satisfaction:\n",
    "\n",
    "Positive aspects:\n",
    "\n",
    "1. Incredibly warm and suitable for cold weather conditions.\n",
    "2. Multiple pockets providing ample storage space.\n",
    "3. Attractive design and aesthetics.\n",
    "4. Comfortable and good fit for some customers.\n",
    "\n",
    "Negative aspects:\n",
    "\n",
    "1. Over-engineered, bulky, and may not be suitable for everyday wear.\n",
    "2. Sizing issues, with some customers reporting that the jacket runs large.\n",
    "3. Poor customer service experiences, including shipping issues and unhelpful responses.\n",
    "4. Expensive and potentially not worth the price for some customers.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bc2c15",
   "metadata": {},
   "source": [
    "Now is your turn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56471cf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5a41fb",
   "metadata": {},
   "source": [
    "### 2. Ask for structured output\n",
    "\n",
    "Sometimes can be helpful to ask the model for structured outputs such as `JSON`, `XML`, or any other format supported by the model. Using this tactic reduces the complexity of the model response while making it easier to process the response using Python or any other coding language. So let's go back to our initial prompt and update it asking the model to format the output in `JSON` format with two lists: positive and negative aspects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc6cd9",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a18ba59",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a list of product reviews delimited by <>.\n",
    "Analyze each reviews and provide an overview of customers satisfaction, listing \n",
    "positive and negative aspects reported by customers. \n",
    "\n",
    "Provide them in JSON format with the following keys: \n",
    "positive_aspects, negative_aspects.\n",
    "\n",
    "<{reviews}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78223491",
   "metadata": {},
   "source": [
    "The new model response is more specific, with less room for creativity and easy to read from code. This introduces another critical aspect of prompt engineering: Balance specificity with creativity. While being specific is crucial, it is also important to allow room for the model to demonstrate its creative capabilities. Striking the right balance between specificity and creativity can lead to more engaging and useful results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c55914",
   "metadata": {},
   "source": [
    "**Response 🤖** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40856f69",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "  \"positive_aspects\": [\n",
    "    \"Incredibly warm\",\n",
    "    \"Many pockets\",\n",
    "    \"High-quality details\",\n",
    "    \"Comfortable\",\n",
    "    \"Stylish\",\n",
    "    \"Excellent appearance\"\n",
    "  ],\n",
    "  \"negative_aspects\": [\n",
    "    \"Over-engineered\",\n",
    "    \"Bulky\",\n",
    "    \"Limited mobility\",\n",
    "    \"Sizing issues\",\n",
    "    \"Expensive\",\n",
    "    \"Poor customer service\",\n",
    "    \"Shipping issues\",\n",
    "    \"Dirty and damaged upon arrival\"\n",
    "  ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea602b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a527e7b",
   "metadata": {},
   "source": [
    "### 3. Check whether conditions are satisfied\n",
    "When designing application prompts based on user input, it is good practice to ensure that the input meets certain minimal criteria necessary to complete the instructions. If not, return a known error message. By doing so, we can guarantee that the model responds in a controlled manner to any input that exceeds the scope for which the prompt and the application were initially designed.\n",
    "\n",
    "For instance, in the subsequent shopper review, another brand's jacket is being promoted. To prevent it from skewing our analysis, we will modify our prompt to automatically ignore this type of reviews.\n",
    "\n",
    "\n",
    "(Additional Review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e4a62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_review = \"\"\"\n",
    "---\n",
    "1 Star\n",
    "Not a real Jacket\n",
    "\n",
    "Looking for a real jacket, suitable for any weather condition and ready for the highest demands? Check out our McMurdo Men’s McMurdo Parka, the best value for money on the market.\n",
    "\"\"\"\n",
    "\n",
    "reviews += additional_review"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedc9215",
   "metadata": {},
   "source": [
    "First, iterate on our last prompt to distinguish between valid and invalid customer reviews. The way to do this is to explicitly tell or show the model what is a valid product review. In the following prompt we are instructing the model to ignore unrelated reviews. I encourage you to do your own test with the model.\n",
    "\n",
    "\n",
    "(Intermediate Prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e15a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a product review about a Columbia Jacket, delimited by \n",
    "<>.\n",
    "\n",
    "If the text is related to the product, provide an overview of customer satisfaction, listing positive and negative aspects reported by the customer. \n",
    "\n",
    "If the text is not related to the product, then simply write \\\"The provided review is not related to the product or is a Spam.\\\"\n",
    "\n",
    "{reviews}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feecdd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74b288b",
   "metadata": {},
   "source": [
    "Now we know how to validate a condition over the input, let's combine this with the original prompt to add an ignored reviews counter into the `JSON` response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bf311a",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50a59db",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a list of product reviews about a Columbia Jacket delimited by <>.\n",
    "Analyze each reviews and provide an overview of customers satisfaction, listing \n",
    "positive and negative aspects reported by customers. \n",
    "\n",
    "It may happen that some of the reviews are not strictly related to the product, in that case ignore the review, \n",
    "update a counter of ignored reviews that you will display with the rest of the output and continue with the next review. \n",
    "\n",
    "Provide them in JSON format with the following keys: \n",
    "positive_aspects, negative_aspects, ignored.\n",
    "\n",
    "<{reviews}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1db970",
   "metadata": {},
   "source": [
    "**Response 🤖** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b082df63",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "{\n",
    "  \"positive_aspects\": [\n",
    "    \"Incredibly warm\",\n",
    "    \"Many pockets\",\n",
    "    \"Great details\",\n",
    "    \"Comfy\",\n",
    "    \"Looks great\",\n",
    "    \"Excellent appearance\"\n",
    "  ],\n",
    "  \"negative_aspects\": [\n",
    "    \"Over engineered\",\n",
    "    \"Lack of mobility\",\n",
    "    \"Too large in length\",\n",
    "    \"Bulky\",\n",
    "    \"Sizing issues\",\n",
    "    \"Poor customer service\",\n",
    "    \"Delayed shipping\",\n",
    "    \"Package received damaged and dirty\"\n",
    "  ],\n",
    "  \"ignored\": 1\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cabd0d",
   "metadata": {},
   "source": [
    "Far better… Notice that this prompt is relying on the current knowledge of the model to distinguish a review concerning a Columbia Jacket (valid) from any other text (invalid). For this example is enough but there are more robust techniques to teach the model on distinguish between valid and invalid inputs. The few-shot prompting tactic covered in the next section is a good example. But first, feel free to add more review examples to push model limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d56d836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc1627",
   "metadata": {},
   "source": [
    "### 4. Few-shot prompting\n",
    "\n",
    "In some scenarios, providing examples of successful executions of the task and then asking the model to perform it is a smart strategy for more accurate and predictable results. To illustrate this tactic, lets momentarily think in a related functionality in the e-commerce website. Imagine that now we want to automatically generate a response message for each customer review.\n",
    "\n",
    "To do this, we are going to tell the model how to respond to some typical examples of product reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225f5085",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a794b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a list of product reviews delimited by <>.\n",
    "Analyze each review and provide a personalized answer for each one, based on the content of the review.\n",
    "\n",
    "Here is a few examples:\n",
    "\n",
    "---\n",
    "Review: I am very happy with this jacket, it is worth every penny\n",
    "Response: Thank you for your review, we are very happy that you are satisfied with our products.\n",
    "\n",
    "---\n",
    "Review: I normally wear Columbia size L jackets, this in L is like an XL. I reordered the M… unfortunately the blue was not longer available =(\n",
    "Response: Thank you for your review! We are continuously working to bring you a better shopping experience and innovative products that will surprise you.\n",
    "\n",
    "---\n",
    "Review: Nobody is truly going to be wearing this unless your in the frozen tundra…. Zero mobility way to large in length… Absolute waste…\n",
    "Response: Thank you for your review, the feedback from our users represents an invaluable asset for us that allows us to work to improve each product and bring you a better shopping experience. \n",
    "\n",
    "Provide them in JSON format with the following keys: review, response. Where review is the first 10 words of the customer \n",
    "review to easy identify it and response is the generated response.\n",
    "\n",
    "<{reviews}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0906e346",
   "metadata": {},
   "source": [
    "Note that the model generates different responses, based on the user message, with a tone similar to the provided examples. You can extend the teaching examples list to show the model how to act under different circumstances. Also, note the model is not strictly following our specifications. The review summary is five words in length instead of ten. This may indicates that we still need to work on our prompt to be clear and specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db43ff",
   "metadata": {},
   "source": [
    "**Response 🤖** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3091f261",
   "metadata": {},
   "source": [
    "```python\n",
    "{\n",
    "  \"1\": {\n",
    "    \"review\": \"This is an over engineered waste\",\n",
    "    \"response\": \"Thank you for your review. We're sorry that the product didn't meet your expectations. We value your feedback and will consider it for future product improvements.\"\n",
    "  },\n",
    "  \"2\": {\n",
    "    \"review\": \"Nobody is truly going to\",\n",
    "    \"response\": \"Thank you for your review. We apologize if the product didn't meet your needs. We appreciate your feedback and will use it to improve our products and ensure they cater to a wider range of customers.\"\n",
    "  },\n",
    "  \"3\": {\n",
    "    \"review\": \"Incredibly warm! You won’t\",\n",
    "    \"response\": \"Thank you for your review! We're glad to hear that you're happy with the warmth of the jacket. We hope it continues to serve you well in the colder months!\"\n",
    "  },\n",
    "  \"4\": {\n",
    "    \"review\": \"I am a 5”4 woman\",\n",
    "    \"response\": \"Thank you for your review! It's great to hear that the jacket has been keeping you warm and you're enjoying the many pockets. We appreciate your feedback and hope the jacket continues to serve you well!\"\n",
    "  },\n",
    "  \"5\": {\n",
    "    \"review\": \"Absolutely way too bulky and\",\n",
    "    \"response\": \"Thank you for your review. We apologize for the inconvenience you've experienced with the sizing and bulkiness of the jacket. Your feedback is important to us, and we'll take it into consideration for future product improvements.\"\n",
    "  },\n",
    "  \"6\": {\n",
    "    \"review\": \"I love Star Wars and\",\n",
    "    \"response\": \"Thank you for your review. We're sorry to hear about the issues you experienced with shipping and customer service. Your feedback is valuable to us, and we'll work on improving our processes to provide a better experience in the future.\"\n",
    "  },\n",
    "  \"7\": {\n",
    "    \"review\": \"A very nice parka!! Comfy\",\n",
    "    \"response\": \"Thank you for your review! We're happy to hear that you're enjoying the comfort and warmth of the parka. We hope it continues to serve you well in the colder months!\"\n",
    "  },\n",
    "  \"8\": {\n",
    "    \"review\": \"Warmest jacket I've ever had.\",\n",
    "    \"response\": \"Thank you for your review! It's great to hear that this is the warmest jacket you've ever had. We hope you continue to enjoy its warmth and style during the colder months!\"\n",
    "  },\n",
    "  \"9\": {\n",
    "    \"review\": \"The jacket looks really nice\",\n",
    "    \"response\": \"Thank you for your review. We apologize for the issues you experienced with sizing. We appreciate your feedback and will use it to improve our sizing charts to better accommodate customers with different body types.\"\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6e9bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7bf790c",
   "metadata": {},
   "source": [
    "## Principle 2: Give the model time to think\n",
    "If you provide the model a problem which is to complex, the model might need more time to think in a proper answer. This means spending more computational time to generate the response and also use more tokens in both input and output response. You can think of this as asking the model to organize its reasoning process in a particular way, for example by following a series of steps, or performing certain tasks before generating a final answer. But above all, use more tokens in the response and process it for more computation cycles.\n",
    "\n",
    "\n",
    "Here are some tactics that can be used to instruct the model to think longer before generate a response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b611a",
   "metadata": {},
   "source": [
    "### 1. Specify the steps to complete a task\n",
    "The first tactic is to provide the model with a comprehensive list of steps to solve the task, the more specific the steps, the better. Let's go back to our shopper reviews and rewrite the model prompt to do both: get customer insights and respond to reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b71503",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6910ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "Your task is to perform the following actions: \n",
    "1 - Analyze the following product reviews delimited by \n",
    "  <> and summary each review with 1 sentence.\n",
    "2 - Categorize the opinion according to the following categories: super happy, happy, neutral, upset\n",
    "3 - Provide an overview of the customer satisfaction, listing positive and negative aspects mentioned in the review\n",
    "4 - Provide a personalized answer for each one, based on the content of the review.\n",
    "5 - Output a json list object that contains the \n",
    "  following keys for each review: summary, category, insights, answer.\n",
    "\n",
    "Text: <{reviews}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d9c054",
   "metadata": {},
   "source": [
    "I had to reduce the number of input reviews to narrow down the response length, as the response length limit was leaving some unparsed reviews in the model output. Despite that, you can verify that the model generates more accurate review responses by understanding each review fairly well and extracting valuable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2040150",
   "metadata": {},
   "source": [
    "**Response 🤖** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5294d38",
   "metadata": {},
   "source": [
    "```python\n",
    "[\n",
    "  {\n",
    "    \"summary\": \"Reviewer thinks the jacket is an over-engineered waste of money with zero mobility.\",\n",
    "    \"category\": \"upset\",\n",
    "    \"insights\": {\n",
    "      \"positive\": [],\n",
    "      \"negative\": [\n",
    "        \"over engineered\",\n",
    "        \"zero mobility\",\n",
    "        \"large in length\"\n",
    "      ]\n",
    "    },\n",
    "    \"answer\": \"We're sorry to hear that you found the jacket to be over-engineered and uncomfortable. We appreciate your feedback and will take it into consideration for future improvements.\"\n",
    "  },\n",
    "  {\n",
    "    \"summary\": \"Reviewer finds the jacket incredibly warm.\",\n",
    "    \"category\": \"super happy\",\n",
    "    \"insights\": {\n",
    "      \"positive\": [\n",
    "        \"incredibly warm\"\n",
    "      ],\n",
    "      \"negative\": []\n",
    "    },\n",
    "    \"answer\": \"We're thrilled to know that you're enjoying the warmth of the jacket! Thanks for sharing your positive experience.\"\n",
    "  },\n",
    "  {\n",
    "    \"summary\": \"Reviewer loves the warmth, pockets, and details of the jacket.\",\n",
    "    \"category\": \"super happy\",\n",
    "    \"insights\": {\n",
    "      \"positive\": [\n",
    "        \"warm\",\n",
    "        \"many pockets\",\n",
    "        \"amazing details\"\n",
    "      ],\n",
    "      \"negative\": []\n",
    "    },\n",
    "    \"answer\": \"We're glad that you love the jacket's warmth, pockets, and attention to detail. It's great to know that it has served you well during the cold weather in Detroit!\"\n",
    "  },\n",
    "  {\n",
    "    \"summary\": \"Reviewer complains about the jacket being too bulky, not fitting as described, and receiving a dirty product.\",\n",
    "    \"category\": \"upset\",\n",
    "    \"insights\": {\n",
    "      \"positive\": [],\n",
    "      \"negative\": [\n",
    "        \"too bulky\",\n",
    "        \"does not fit as described\",\n",
    "        \"dirty\"\n",
    "      ]\n",
    "    },\n",
    "    \"answer\": \"We apologize for the inconvenience caused by the sizing and cleanliness issues. We appreciate your feedback and will work on improving these aspects. Please feel free to return the jacket for a refund or exchange.\"\n",
    "  },\n",
    "  {\n",
    "    \"summary\": \"Reviewer loves the parka, saying it's comfy, warm, and looks great.\",\n",
    "    \"category\": \"super happy\",\n",
    "    \"insights\": {\n",
    "      \"positive\": [\n",
    "        \"comfy\",\n",
    "        \"warm\",\n",
    "        \"looks great\"\n",
    "      ],\n",
    "      \"negative\": []\n",
    "    },\n",
    "    \"answer\": \"We're happy to hear that you love the parka's comfort, warmth, and style. Thank you for your positive review!\"\n",
    "  },\n",
    "  {\n",
    "    \"summary\": \"Reviewer thinks the jacket is the warmest they've ever had, with lots of pockets and an excellent look.\",\n",
    "    \"category\": \"super happy\",\n",
    "    \"insights\": {\n",
    "      \"positive\": [\n",
    "        \"warmest jacket\",\n",
    "        \"lots of pockets\",\n",
    "        \"looks excellent\"\n",
    "      ],\n",
    "      \"negative\": []\n",
    "    },\n",
    "    \"answer\": \"We're delighted that you find our jacket to be the warmest you've ever had, and that you appreciate the pockets and style. Thank you for sharing your experience!\"\n",
    "  },\n",
    "  {\n",
    "    \"summary\": \"Reviewer likes the look and warmth of the jacket but finds the sizing chart off.\",\n",
    "    \"category\": \"neutral\",\n",
    "    \"insights\": {\n",
    "      \"positive\": [\n",
    "        \"nice looking\",\n",
    "        \"warm\"\n",
    "      ],\n",
    "      \"negative\": [\n",
    "        \"sizing chart off\"\n",
    "      ]\n",
    "    },\n",
    "    \"answer\": \"We appreciate your feedback on the jacket's look and warmth. We apologize for the sizing issue and will work to improve the accuracy of our sizing chart. Please consider exchanging for a better fit if needed.\"\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc69fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13ca91",
   "metadata": {},
   "source": [
    "### 2. Instruct the model to work out its own solution before rushing to a conclusion\n",
    "The second tactic is similar to the previous one but in addition ask the model to first reach a solution by himself before rushing to a conclusion. This is similar to instead of instruct the model to say if something is correct or incorrect based on the context, instruct the model to first work on his own solution based on the context and then compare it with the input solution.\n",
    "\n",
    "For example if the task is validate students answers for a simple math problem like `2 + x = 4` then `x=?`, instruct the model to first find a solution and then compare it with the student’s solution.\n",
    "\n",
    "\n",
    "To illustrate the power of this tactic, we will explore another feature of the e-commerce website: processing returns and refunds tickets. The idea is to give the model a list of actual shopper's refund demands, to process and evaluate if a refund applies based on the company policy. Here are six examples of refund requests for various reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfe166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "refunds = \"\"\"\n",
    "---\n",
    "User: Jack\n",
    "Text: The description of the product stated that the socks were made primarily of cotton with a bit of spandex. The product I received indicated that the socks were made of primarily of polyester and a tiny bit of spandex. I wanted cotton socks so I returned the product. Several attempts to contact the supplier went unanswered and I still have not received a refund on my credit card. I need a solution and want a refund.\n",
    "\n",
    "---\n",
    "User: Tom\n",
    "Text: When I received my purchase, the package was open and significantly damaged. After unpacking it, I found that the shirt was dirty and had several scratches. I want to return and have a new unit sent to me.\n",
    "\n",
    "--- \n",
    "User: Bob\n",
    "Text: I ordered an XL just as my other Columbia jackets but this one is basically a XXL. The jacket is also extremely long for no reason. I want to return it.\n",
    "\n",
    "--- \n",
    "User: Shara\n",
    "Text: The jacket looks really nice and is warm but the sizing chart is completely off. I would normally be a large to extra large because of wide shoulders and back (I matched the large I'm the size chart). However, after using it a week I can say I need to go a size down or maybe even 2. Can I change it for a medium size?\n",
    "\n",
    "---\n",
    "User: Sasha\n",
    "Text: This jacket is incredibly warm and solidly built. However, they charged my credit card twice, so I am writing to get a refund of the second charge. Thanks in advance.\n",
    "\n",
    "---\n",
    "\n",
    "User: Chris\n",
    "Text: I bought this backpack for my children because the capacity and solid construction look. However after 2 weeks of school is completely demolished. Poor quality materials and broken zips. I want a refund.\n",
    "\n",
    "---\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec55de",
   "metadata": {},
   "source": [
    "For those who have not had the joy of receiving a damaged product, the returns and refunds conditions are not particularly easy to read, with plenty of rules for specific cases and fine print. For example, let's imagine that the return and refund policy of our e-commerce site is reduced to the following four rules (in a real scenario, it would be much more than four).\n",
    "\n",
    "1. The customer received the product with damages or in poor conditions\n",
    "2. The customer received the wrong size or other product. The product must be still in the original package without usage.\n",
    "3. The customer is not satisfied with the product and wishes to change it. The product must be still in the original package without usage.\n",
    "4. The customer was charged more than once in his credit card.\n",
    "\n",
    "Under these conditions, Shara and Chris's requests are not eligible for a refund since they took the purchase out of its original packaging and used it for a while. The conditions clearly state that the products must be in their original packaging and must not be used. All other requests apply for a refund.\n",
    "\n",
    "Now we have the rules, let's design a prompt to instruct the model for processing refund requests. The idea is to follow a step-by-step reasoning process and reach a conclusion based on the rule set instead of just guessing. Feel free to try this prompt by yourself or even modify it to follow a different reasoning process. Notice that this would be also a great case to provide real examples as a part of the prompt, and take advantage of the few-shot learning capabilities of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee5c862",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66884ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be provided with a list of user complaints delimited by <>, who want a refund of their purchase for different reasons. \n",
    "Your task is to determine if corresponds or not, to give the customer a refund based on the company policy.\n",
    "\n",
    "Refunds policies\n",
    "\n",
    "Only apply a refund in the following scenarios:\n",
    "- The customer received the product with damages or in poor conditions\n",
    "- The customer received the wrong size or other product. The product must be still in the original package without usage.\n",
    "- The customer is not satisfied with the product and wishes to change it. The product must be still in the original package without usage.\n",
    "- The customer was charged more than once in his credit card.\n",
    "\n",
    "To solve the problem do the following:\n",
    "\n",
    "1 - Analyze whether the purchase applies the refunds policy based on the reasons given by the customer.\n",
    "2 - Categorize the case in: \"apply\" or \"not apply\" based on your previous judgment.\n",
    "3 - Provide a concise and personalized response with the reasons for your previous solution.\n",
    "5 - Output a json list object that contains the \n",
    "  following keys for each case: category, answer.\n",
    "\n",
    "Tickets: <{refunds}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08bf1c8",
   "metadata": {},
   "source": [
    "As we can see in the answer, GPT-4 correctly solves each case and also provides a more than correct justification pointing out the reasons why it does NOT apply for a refund in the two negative cases. Surely it is necessary to test on a larger and more representative dataset before rushing to the conclusion that LLM can perfectly deal with this task. How ever, the results are very encouraging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c6bd74",
   "metadata": {},
   "source": [
    "**Response 🤖** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c7869e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "[\n",
    "  {\n",
    "    \"category\": \"apply\",\n",
    "    \"answer\": \"Dear Jack, we apologize for the inconvenience caused. Since you received a product with a different material than described and have already returned it, you are eligible for a refund. We will process your refund as soon as possible.\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"apply\",\n",
    "    \"answer\": \"Dear Tom, we're sorry to hear that the product arrived damaged and dirty. As per our refund policy, you are eligible for a replacement. Please return the damaged shirt, and we will send you a new one.\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"apply\",\n",
    "    \"answer\": \"Dear Bob, we apologize for the sizing issue you experienced. As you haven't used the jacket and it's still in the original package, you can return it. We will process your refund or exchange it for the correct size upon receiving the returned item.\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"not apply\",\n",
    "    \"answer\": \"Dear Shara, we appreciate your feedback on the sizing chart. However, since you have already used the jacket for a week, we cannot process an exchange or refund. We recommend referring to the sizing chart and contacting us before making future purchases to ensure a better fit.\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"apply\",\n",
    "    \"answer\": \"Dear Sasha, we apologize for the double charge on your credit card. We will refund the second charge immediately. Thank you for bringing this to our attention.\"\n",
    "  },\n",
    "  {\n",
    "    \"category\": \"not apply\",\n",
    "    \"answer\": \"Dear Chris, we're sorry to hear that the backpack didn't meet your expectations. However, as the item has been used for two weeks, our refund policy does not apply in this case. We encourage you to read our refund policies carefully before making future purchases.\"\n",
    "  }\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef4b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54888c6d",
   "metadata": {},
   "source": [
    "## Principle 3: Balance specificity with creativity\n",
    "A critical aspect when you write prompts is balancing specificity with creativity. While being specific is crucial, it is also important to allow room for the model to demonstrate its creative capabilities. Striking the right balance between specificity and creativity can lead to more engaging and useful results. However, giving too much space to creativity can be a bad idea for some poorly tested or unstable LLMs, making it easy for hallucinations, hate speech, or racism to appear in the response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe89d20",
   "metadata": {},
   "source": [
    "### 1. The Tolkien’s Method\n",
    "J. R. R. Tolkien, a British world-famous writer well known for being the author of the fantasy novel trilogy The Lord of The Ring, used to trust a narrative style in which details abound and delves into secondary stories and legends plenty of details, all with the aim that the reader gets a precise and faithful image of his fantastic middle earth. A simple but effective technique to help the reader to picture landscapes and characters in a similar direction that Tolkien visualized. Several writers have done the same when describing their fantastic universes: George R. R. Martin, J. K Rowling, Dan Simmons, etc.\n",
    "\n",
    "Analogously, we can play the role of a writer, like Tolkien, when design prompts. In this way, providing exhaustive details and context, to orient the model (the reader) to picture exactly what we have in mind. This strategy, which I like to call the Tolkien’s Method, allows us to obtain super precise results while limiting model creativity.\n",
    "\n",
    "For example, suppose we want to develop a new feature to automatically suggests a product description based on the product manufacturer's info and speed up the marketing team's daily work. Let's take the Columbia jacket as an example and build a prompt to complete this task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953dc05",
   "metadata": {},
   "source": [
    "**Product Info 🧥**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c406c",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Columbia Men's Republic Parka (Obi-Wan Kenobi™)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db68c958",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0f21c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be given a product name delimited by <>.\n",
    "Your task is to provide a product description, to be used on the product listing page, within an e-commerce website.\n",
    "\n",
    "<{product}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5130c7a",
   "metadata": {},
   "source": [
    "**Response 🤖** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85faf34",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "The Columbia Men's Republic Parka (Obi-Wan Kenobi™) is the perfect intersection of fashion and fandom for all Star Wars enthusiasts. This special edition parka, inspired by the iconic Jedi Master Obi-Wan Kenobi, is designed to provide superior comfort and warmth while embracing the essence of your favorite Star Wars character.\n",
    "\n",
    "Made from high-quality, water-resistant materials, this parka is designed to withstand diverse weather conditions, making it perfect for adventures in both urban and outdoor settings. The adjustable hood is lined with faux-fur trim reminiscent of Obi-Wan's desert attire, and the jacket is available in a Jedi-inspired color palette, adding an authentic Star Wars touch.\n",
    "\n",
    "The parka features multiple pockets, providing ample storage for your essentials. The full-length zip with a snap-button storm flap ensures additional weather protection, and the adjustable cuffs and hem provide a customizable fit for optimal comfort.\n",
    "\n",
    "As a part of Columbia's Star Wars collection, this Republic Parka showcases an embroidered Obi-Wan Kenobi silhouette patch, which will proudly proclaim your affinity for the legendary Jedi. Whether you're off on an interstellar adventure or simply braving the cold, the Columbia Men's Republic Parka (Obi-Wan Kenobi™) ensures that the Force will always be with you.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf99485",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266139c8",
   "metadata": {},
   "source": [
    "Giving the model absolute freedom, the response obtained is quite impressive. The product description mentions actual Columbia technologies and incorporates accurate facts about the Star Wars Saga. However, what could happen if the product is new, and the model does not know facts about it? For example, a new Columbia Parka inspired by the Star Wars saga aesthetics and Donald Trump politics character (you can try it as an exercise).\n",
    "\n",
    "An alternative approach is to restrict the model response by providing details about the response structure, use of language, and a source of facts to use. For example, let’s change the prompt using Tolkien’s method to restrict the possible description. First, let's include a complete sheet with product information to be used as a source of facts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4621cd",
   "metadata": {},
   "source": [
    "**Product Info 🧥**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfd04445",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"\"\"\n",
    "Men's Republic Parka (Obi-Wan Kenobi™)\n",
    "\n",
    "Details\n",
    "THE JEDI™ WAY\n",
    "Seize the high ground this winter with a special-edition hooded parka built to keep you warm in frigid conditions and crafted for Star Wars: The Clone Wars™ fans and outdoor enthusiasts alike.\n",
    "MASTER THE ELEMENTS\n",
    "Inspired by Obi-Wan Kenobi™ and his adventures on the ice planet Orto Plutonia™, the waterproof-breathable shell keeps you dry and protected while 650-fill insulation and advanced thermal-reflective lining team up against the cold.\n",
    "FOR THE REPUBLIC\n",
    "Fan-favorite details include Jedi and Republic insignia along with an internal “blood chit” label coded in Aurebesh™, the universal written language of the Star Wars galaxy. Additional features include security, hand, chest, and cargo pockets along with comfort cuffs and an adjustable hood (with removable faux fur) and hem.\n",
    "\n",
    "Omni-Tech™ waterproof/breathable critically seam sealed\n",
    "Omni-Heat™ Infinity advanced thermal reflective\n",
    "650 fill power down insulation, RDS certified\n",
    "Attached, adjustable hood\n",
    "Interior security pocket\n",
    "Internal mesh pockets\n",
    "Zippered hand pockets\n",
    "Dual entry chest pockets\n",
    "Cargo pockets\n",
    "Drawcord adjustable hem\n",
    "Drop tail\n",
    "Removable faux fur\n",
    "Comfort cuff with thumb hole\n",
    "Center Back Length: 33\"\n",
    "Imported\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c41274",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9317d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be given a product name and complete description delimited by <>.\n",
    "\n",
    "Your task is to provide a product description, to be used on the product listing page, within an e-commerce website, based only on the information present on the product \n",
    "details. Do not use facts about the product outside the provided description. Also, ensure description is at most 150 words long and contains common used words in english. Regarding the structure, starts with a catchy sentence that captures the spirit of the \n",
    "product and then continues mentioning possible uses of the product in real life and use this to highlight the main characteristics of the product. Ends the description with a final catchy sentence that motivates the reader to go to the nearest store to buy it.\n",
    "\n",
    "<{product}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dca6fc4",
   "metadata": {},
   "source": [
    "**Response 🤖** &nbsp;&nbsp;<span class=\"label success\">GPT-4</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a094d193",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "Embark on your everyday adventures with the Men's Republic Parka (Obi-Wan Kenobi™), designed to withstand the coldest winters and perfect for Star Wars: The Clone Wars™ fans and outdoor enthusiasts. Channel your inner Jedi with this parka, inspired by Obi-Wan Kenobi's exploits on the frosty Orto Plutonia™. Experience supreme warmth and comfort, thanks to the 650-fill insulation and Omni-Heat™ Infinity advanced thermal-reflective lining that works diligently against the cold.\n",
    "\n",
    "Whether you're braving the city streets on a frosty day or exploring snowy landscapes, this parka ensures ultimate protection. The waterproof-breathable shell, combined with an adjustable hood, keeps you dry, while the cargo, chest, and hand pockets provide ample storage. Subtle Star Wars-inspired details such as Jedi and Republic insignias and an internal Aurebesh™ label elevate your style quotient.\n",
    "\n",
    "So, don your parka and seize the high ground this winter - for the Republic! Your next cold-weather adventure awaits\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad459f0",
   "metadata": {},
   "source": [
    "As you can see in the answer, the more info you provide in the input, context, and expected output, the more predictable the result is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25a7f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09467e0",
   "metadata": {},
   "source": [
    "### 2. Temperature and Top P Parameters\n",
    "In addition to the improved transformer architecture and massive unsupervised training data, better decoding methods have also played an important role in achieving the impressive results in language generation demonstrated by models like `GPT-3`. If you are not familiar with decoding methods I strongly recommend you to read this [post](https://huggingface.co/blog/how-to-generate?ref=hackernoon.com) using different decoding methods for language generation with Transformers. In short, auto regressive language models use different decoding methods to select the next token after inferring the individual probabilities, the most popular are Greedy search, Beam search, Top-K sampling, and Top-p sampling. Particularly `GPT-3` and `GPT-4` use Top-p sampling and understanding how to use it can help you to get more expressive and creative results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5c677b",
   "metadata": {},
   "source": [
    "<img src=\"images/3.jpeg\" alt=\"Word Selection\" width=\"450\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e6708e",
   "metadata": {},
   "source": [
    "#### Top P\n",
    "\n",
    "Top-p sampling, introduced by [Holtzman et al (2019)](https://arxiv.org/abs/1904.09751?ref=hackernoon.com), chooses from the smallest possible set of words whose cumulative probability exceeds the probability p. In other words, if you set  `Top P=0.5`, the model will consider for predicting the next word only the subset of top tokens whose cumulative probability exceeds _p=0.5_ and ignore the rest. Similarly, setting a low value of `Top P` (for instance `0.1`), reduce the list of tokens to be considered for the next word prediction, while setting a high value of `Top P` increase the size of the list.\n",
    "\n",
    "In other words, high value of `Top P` increase randomness on next word prediction, allowing more creative results, while lower values of `Top P` decrease creativity, making the next-token selection more predictable. By default, `Top P` is set to `1.0` on OpenAI models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda66120",
   "metadata": {},
   "source": [
    "<img src=\"images/4.jpeg\" alt=\"Word Selection\" width=\"600\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5370936",
   "metadata": {},
   "source": [
    "To illustrate the effect of the `Top P` parameter, let’s generate a product description based on the product title and tweak this parameter to observe the impact on the generated output.\n",
    "\n",
    "To amplify the effect on the result, we are limiting the output to 20 words long, and running the same prompt through the model four times for each value of `Top P`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba153ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "product = \"Columbia Men's Republic Parka (Obi-Wan Kenobi™)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561e773",
   "metadata": {},
   "source": [
    "**Prompt 💬**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5406e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "You will be given a product name delimited by <>.\n",
    "Your task is to provide a product description, to be used on the product listing page, within an e-commerce website. Use a limit of 20 words in the description\n",
    "\n",
    "<{product}>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcabd883",
   "metadata": {},
   "source": [
    "**Responses 🤖 for Top P = 1 (default)** &nbsp;&nbsp;<span class=\"label info\">GPT-3.5-Turbo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc79a9e",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "(1)\n",
    "Stay warm and stylish with Columbia's Republic Parka inspired by Obi-Wan Kenobi™. Perfect for any outdoor adventure.\n",
    "\n",
    "(2)\n",
    "Stay warm and stylish with this Columbia Men's Republic Parka featuring Obi-Wan Kenobi™ artwork. Perfect for any Star Wars fan.\n",
    "\n",
    "(3)\n",
    "Stay warm and stylish with the Columbia Men's Republic Parka, featuring a unique Obi-Wan Kenobi™ design.\n",
    "\n",
    "(4)\n",
    "Stay warm on Hoth with this Star Wars-inspired parka featuring Omni-Heat™ reflective lining and waterproof fabric.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b237473f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt, top_p=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadb1c83",
   "metadata": {},
   "source": [
    "**Responses 🤖 for Top P = 0.1** &nbsp;&nbsp;<span class=\"label info\">GPT-3.5-Turbo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37070b0e",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "(1)\n",
    "Stay warm on Hoth with this Star Wars-inspired parka featuring Omni-Heat™ reflective lining and waterproof fabric.\n",
    "\n",
    "(2)\n",
    "Stay warm and stylish with this Columbia parka, inspired by Obi-Wan Kenobi™. Perfect for any winter adventure.\n",
    "\n",
    "(3)\n",
    "Stay warm and stylish with this Columbia parka, inspired by Obi-Wan Kenobi™. Perfect for any winter adventure.\n",
    "\n",
    "(4)\n",
    "Stay warm like a Jedi with this Columbia parka, inspired by Obi-Wan Kenobi™. Perfect for cold weather adventures.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693d9e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt, top_p=0.1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ead31",
   "metadata": {},
   "source": [
    "As you can see from model responses comparing the difference between the model output for each of the repetitions, with `Top P = 1` we have more variance on the output, altering several words and in different positions, even the output length changes. On the other hand, the responses for `Top P = 0.1` are more deterministic, with minor variations on some words. In summary, tweak Top P parameter is an effective strategy to change from deterministic results (`Top P` close to `0`) to unpredictable and more creative results (`Top P` close to `1`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e24b528",
   "metadata": {},
   "source": [
    "#### Temperature\n",
    "There is a trick inspired by thermodynamics, to make the distribution of the next words' conditioned probabilities sharper (increasing the likelihood of high-probability words and decreasing the likelihood of low-probability words) by lowering the so-called temperature of the [softmax](https://en.wikipedia.org/wiki/Softmax_function?ref=hackernoon.com#Smooth_arg_max). A temperature parameter close to 1 would mean that the logits are passed through the softmax function without modification. If the temperature is close to zero, the highest probable tokens will become very likely compared to the other tokens. An illustration from [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate?ref=hackernoon.com), reflects pretty well the effect of applying temperature to a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d98d75",
   "metadata": {},
   "source": [
    "<img src=\"images/5.png\" alt=\"Temperature\" width=\"700\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20ffdeb",
   "metadata": {},
   "source": [
    "Enough theory is time to see the `Temperature` parameter in action with the same prompt example.\n",
    "\n",
    "As you can see from model responses comparing the results from each configuration, the effect on the model response is very similar to the effect produced by `Top P` parameter. Using a `Temperature` value close to `0` generates more deterministic results, while using values close to `1` produces more unpredictable and creative results. You can increase `Temperature` till `2.0` to get even more randomness on the results. How ever, I need to warm you that some times values over `1.5` tends to produce hallucinations and nonsense tokens (try yourself)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de390c9",
   "metadata": {},
   "source": [
    "**Responses 🤖 for Temperature = 1.0 (default)** &nbsp;&nbsp;<span class=\"label info\">GPT-3.5-Turbo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96ab2891",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "(1)\n",
    "Stay warm on Hoth or anywhere in the galaxy with this insulated parka, featuring Obi-Wan Kenobi™ inspired design elements.\n",
    "\n",
    "(2)\n",
    "Stay warm on planet Hoth with this Jedi-worthy parka featuring Columbia's signature technology.\n",
    "\n",
    "(3)\n",
    "Stay warm in style with this parka inspired by Obi-Wan Kenobi™, crafted by Columbia for men's ultimate comfort.\n",
    "\n",
    "(4)\n",
    "Stay warm and stylish with the Columbia Men's Republic Parka featuring iconic Star Wars character Obi-Wan Kenobi.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdeac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt, temperature=1.0)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e2cf49",
   "metadata": {},
   "source": [
    "**Responses 🤖 for Temperature = 1.5** &nbsp;&nbsp;<span class=\"label info\">GPT-3.5-Turbo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f09830",
   "metadata": {},
   "source": [
    "```python\n",
    "\"\"\"\n",
    "(1)\n",
    "Battle winter chill in iconic style wearing Columbia Men's Republic Parka, designed with insight from the recognized Jedi warrior Ob, vs Kenobi.\n",
    "\n",
    "(2)\n",
    "Stay warm as you navigate treacherous planets with this Columbia parka inspired by the legendary Jedi Master Obi-Wan.\n",
    "\n",
    "(3)\n",
    "Stay toasty warm on the coldest of days with this stylish jacket inspired by Obi-Wan Kenobi™ from Columbia.\n",
    "\n",
    "(4)\n",
    "Stay warm with this insulated parka inspired by Obi-Wan Kenobi's iconic hooded style. Made by Columbia for dependable quality.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb761a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt, temperature=1.5)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dce5d8d",
   "metadata": {},
   "source": [
    "**Responses 🤖 for Temperature = 0.2** &nbsp;&nbsp;<span class=\"label info\">GPT-3.5-Turbo</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f453fe35",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "```python\n",
    "\"\"\"\n",
    "(1)\n",
    "Stay warm like a Jedi with this Columbia parka featuring Obi-Wan Kenobi™ design. Perfect for cold weather adventures.\n",
    "\n",
    "(2)\n",
    "Stay warm in style with this Columbia parka inspired by Obi-Wan Kenobi™. Perfect for any Star Wars fan.\n",
    "\n",
    "(3)\n",
    "Stay warm in style with the Columbia Men's Republic Parka, inspired by the legendary Jedi Master Obi-Wan Kenobi™.\n",
    "\n",
    "(4)\n",
    "Stay warm and stylish with the Columbia Men's Republic Parka, inspired by Obi-Wan Kenobi™.\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f52c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now is your turn!\n",
    "response = get_completion(prompt, temperature=0.2)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd3a831",
   "metadata": {},
   "source": [
    "OpenAI also offers two extra options called Frequency penalty and Presence penalty that can be used to encourage the model to include new tokens on the response penalizing the repetition on tokens. Read more about this [here](https://community.openai.com/t/difference-between-frequency-and-presence-penalties/2777/3)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe94ebde",
   "metadata": {},
   "source": [
    "## Five Tips & Tricks to Make your Prompt Effective\n",
    "<a class=\"anchor\" id=\"five-tips-and-tricks\"></a>\n",
    "\n",
    "The principles can help you as a general guide to writing clear and effective prompts. In addition, the following tips & tricks can help you optimize your prompts even more.\n",
    "\n",
    "\n",
    "**1- Iteration**\n",
    "\n",
    "There is no such thing as a perfect prompt, so start fast with and simple and imperfect prompt and then iterate, iterate, iterate. You can follow a four steps cycle: (i) write the prompt, (ii) execute it, (iii) analyze the error, (iv) find improvements, and repeat.\n",
    "\n",
    "\n",
    "**2- Be Explicit and Use Objective Language**\n",
    "\n",
    "Words that do not have a clear meaning or whose meaning may be subjective can lead the model to confusion. For example, as part of an instruction, ask to rewrite an input text to make it looks better. What does better mean? Instead, it is preferable to explicitly indicate how we want to improve the text: limit the total length to 50 words, correct grammatical errors, avoid personal language, etc.\n",
    "\n",
    "\n",
    "**3- Itemizing Instructions**\n",
    "\n",
    "Instead of writing a long paragraph with complex instructions, break it down into simple items instructions.\n",
    "\n",
    "\n",
    "**4- Avoid Negation Statements**\n",
    "\n",
    "Try to avoid negation statements, sometimes can lead the model to confusions. Turn them into assertion statements. For example, instead of write _“**don’t** use misogynistic or sexist language expressions”_, write _“**use** inclusive and respectful language”_.\n",
    "\n",
    "\n",
    "**5- Switch words**\n",
    "\n",
    "Some words may be more effective than others in terms of contextualizing and guiding the model in the right direction. If your prompt isn't working, try replacing important words with synonyms, opting for simpler words.\n",
    "\n",
    "\n",
    "A final piece of advice, life is not just about prompting! I mean, equally important is to know well the model you are using, how it works, and what extra parameters it has. For example, we can tune model creativity or robustness by tuning model temperature or Top_P parameter.\n",
    "\n",
    "In conclusion, read the documentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2ebd61",
   "metadata": {},
   "source": [
    "## Wrapping up\n",
    "<a class=\"anchor\" id=\"wrapping-up\"></a>\n",
    "\n",
    "In this first post in the Prompt Engineering 101 series, we introduce an intuitive definition of prompt engineering and address three principles behind building effective prompts:\n",
    "\n",
    "1. Write clear and specific instructions\n",
    "2. Give the model time to think\n",
    "3. Balance specificity with creativity.\n",
    "\n",
    "Throw real input examples and prompts, we covered different tactics to create prompts aligned to these principles, such as using delimiters, asking for structured responses such as JSON, etc. Finally, we played with additional parameters like temperature (included by some of the models like GPT3/GPT-4), which helped us to obtain more interesting results.\n",
    "\n",
    "You can find all the examples, along with prompts, responses, and python scripts to interact directly with the OpenAI API and try your own prompts [here](https://github.com/efviodo/prompt-engineering-101).\n",
    "\n",
    "\n",
    "In the following post, we will see more advanced techniques like Chain-Of-Though and Self-Consistency, as well as different techniques and tools to optimize prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f05c1b7",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "This post is inspired by the outstanding courses of [Andrew Ng](https://www.linkedin.com/in/andrewyng/) and [Isabella Fulford](https://www.linkedin.com/in/isabella-fulford/), as well as the excellent LLM Bootcamp provided by [Charles Frye](https://www.linkedin.com/in/charles-frye-38654abb/), [Sergey Karayev](https://www.linkedin.com/in/sergeykarayev/), and [Josh Tobin](https://www.linkedin.com/in/josh-tobin-4b3b10a9) (both courses are mentioned in the resources section). After completing these learning programs, I found myself eager to delve deeper, exploring academic papers and tutorials. This led me on a journey through the Internet, discerning high-quality resources from junk. I even ordered two books from Amazon on Prompt Engineering and Generative AI for Artwork, which turned out to be poorly written and a complete waste of money. After several weeks of intense work, headaches and several cups of coffee, I found myself with a collection of resources of a great value about Prompt Engineering. In the spirit of helping others in their Prompt Engineering journey, I decided to share my experience writing this series of posts.\n",
    "\n",
    "\n",
    "I hope they are helpful and you enjoy them as much as I did writing them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdc31a9",
   "metadata": {},
   "source": [
    "## References\n",
    "<a class=\"anchor\" id=\"references\"></a>\n",
    "\n",
    "* [Prompt Engineering 101: Prompts, Outputs and Notebooks with code](https://github.com/efviodo/prompt-engineering-101)\n",
    "\n",
    "* [ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)\n",
    "\n",
    "* [The Full Stack LLM Bootcamp](https://fullstackdeeplearning.com/llm-bootcamp/)\n",
    "\n",
    "* [OpenAI Playground](https://platform.openai.com/playground)\n",
    "\n",
    "* [OpenAI Cookbook](https://github.com/openai/openai-cookbook)\n",
    "\n",
    "* [How to generate text: using different decoding methods for language generation with Transformers](https://huggingface.co/blog/how-to-generate)\n",
    "\n",
    "* [Holtzman, A., Buys, J., Du, L., Forbes, M., & Choi, Y. (2019). The curious case of neural text degeneration. arXiv preprint arXiv:1904.09751.](https://arxiv.org/abs/1904.09751)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prompt-engineering",
   "language": "python",
   "name": "prompt-engineering"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
